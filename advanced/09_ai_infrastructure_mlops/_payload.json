[{"data":1,"prerenderedAt":516},["ShallowReactive",2],{"advanced-09_ai_infrastructure_mlops":3},{"id":4,"title":5,"body":6,"description":501,"extension":502,"meta":503,"navigation":504,"path":505,"seo":506,"stem":507,"tags":508,"__hash__":515},"advanced/advanced/09_ai_infrastructure_mlops.md","AI Infrastructure & MLOps",{"type":7,"value":8,"toc":490},"minimark",[9,14,33,37,41,283,288,299,303,323,327,332,362,367,390,395,453,458,484,487],[10,11,13],"h2",{"id":12},"learning-objectives","Learning Objectives",[15,16,17,21,24,27,30],"ul",{},[18,19,20],"li",{},"Understand the MLOps lifecycle and how it extends traditional DevOps practices",[18,22,23],{},"Serve machine learning models in production with Kubernetes and GPU scheduling",[18,25,26],{},"Build CI/CD pipelines for model versioning, testing, and deployment",[18,28,29],{},"Monitor model performance in production: latency, throughput, drift, and cost",[18,31,32],{},"Manage the cost of GPU compute as an engineering discipline",[10,34,36],{"id":35},"requirements","Requirements",[38,39,40],"p",{},"You are required to build and operate the infrastructure for running AI/ML workloads in production. You do not need to train models - the focus is entirely on operating them:",[42,43,44,85,138,184,225],"ol",{},[18,45,46,50],{},[47,48,49],"strong",{},"MLOps Architecture and Model Registry",[15,51,52,66,77],{},[18,53,54,55],{},"Deploy MLflow as a model registry and experiment tracker:\n",[15,56,57,60,63],{},[18,58,59],{},"Configure a PostgreSQL backend for metadata",[18,61,62],{},"Configure S3 or GCS for artifact storage (model files, datasets)",[18,64,65],{},"Register at least two versions of a pre-trained model (use any public model from Hugging Face)",[18,67,68,69],{},"Document the MLOps lifecycle:\n",[15,70,71,74],{},[18,72,73],{},"Model development → experiment tracking → model registration → staging → production",[18,75,76],{},"How this differs from and integrates with the existing software delivery pipeline",[18,78,79,80],{},"Define model promotion criteria:\n",[15,81,82],{},[18,83,84],{},"A model is promoted to production only if: accuracy > baseline, latency p95 \u003C 200ms, no regression on a validation dataset",[18,86,87,90],{},[47,88,89],{},"Model Serving Infrastructure",[15,91,92,115,132,135],{},[18,93,94,95],{},"Deploy a model serving stack on Kubernetes:\n",[15,96,97,103,109],{},[18,98,99,102],{},[47,100,101],{},"Option A",": BentoML - package and serve a Hugging Face model as an HTTP API",[18,104,105,108],{},[47,106,107],{},"Option B",": Triton Inference Server - serve an ONNX or TensorRT model",[18,110,111,114],{},[47,112,113],{},"Option C",": Seldon Core or KServe - full model serving platform on Kubernetes",[18,116,117,118],{},"Configure the serving deployment with:\n",[15,119,120,123,126,129],{},[18,121,122],{},"Readiness and liveness probes",[18,124,125],{},"Resource requests/limits (CPU for non-GPU workloads; document GPU requirements if relevant)",[18,127,128],{},"Horizontal scaling based on inference request queue depth (KEDA)",[18,130,131],{},"Rolling deployment strategy for model updates",[18,133,134],{},"Expose the model via an API Gateway with rate limiting and authentication",[18,136,137],{},"Verify the serving endpoint: send inference requests and validate responses",[18,139,140,143],{},[47,141,142],{},"GPU Scheduling (Conceptual + Practical where possible)",[15,144,145,164,181],{},[18,146,147,148],{},"If GPU nodes are available (GKE Autopilot with GPU, AWS EC2 g4dn, or local GPU):\n",[15,149,150,153,161],{},[18,151,152],{},"Deploy the NVIDIA Device Plugin for Kubernetes",[18,154,155,156,160],{},"Schedule a model inference Pod with GPU resource requests (",[157,158,159],"code",{},"nvidia.com/gpu: 1",")",[18,162,163],{},"Measure inference latency on GPU vs. CPU",[18,165,166,167],{},"If GPU nodes are not available:\n",[15,168,169,175,178],{},[18,170,171,172],{},"Document the complete GPU scheduling setup in ",[157,173,174],{},"gpu-infrastructure.md",[18,176,177],{},"Use a CPU-optimized quantized model (GGUF format with llama.cpp or similar) as a substitute",[18,179,180],{},"Document cost comparison: GPU instance cost vs. CPU instance cost for equivalent throughput",[18,182,183],{},"Configure time-slicing or MIG (Multi-Instance GPU) for shared GPU utilization documentation",[18,185,186,189],{},[47,187,188],{},"CI/CD for Models",[15,190,191,211],{},[18,192,193,194],{},"Implement a model deployment pipeline triggered on model registry promotion:\n",[15,195,196,199,202,205,208],{},[18,197,198],{},"Pull the new model version from MLflow registry",[18,200,201],{},"Run automated evaluation: inference latency test, accuracy on a held-out dataset",[18,203,204],{},"Canary deploy: route 10% of traffic to the new model, 90% to the current production model",[18,206,207],{},"Monitor for 30 minutes: if no degradation, promote to 100%",[18,209,210],{},"Automatic rollback if p95 latency degrades > 30% or error rate > 1%",[18,212,213,214],{},"Implement model versioning in the serving layer:\n",[15,215,216,219,222],{},[18,217,218],{},"Multiple model versions can run simultaneously",[18,220,221],{},"Traffic split is configurable without redeployment",[18,223,224],{},"Each version is independently observable",[18,226,227,230],{},[47,228,229],{},"Model Observability and Cost Management",[15,231,232,255,269],{},[18,233,234,235],{},"Deploy a monitoring stack for model inference:\n",[15,236,237,243,249],{},[18,238,239,242],{},[47,240,241],{},"Request metrics",": requests/second, p50/p95/p99 latency, error rate per model version",[18,244,245,248],{},[47,246,247],{},"Resource metrics",": CPU/GPU utilization, memory usage, batch size distribution",[18,250,251,254],{},[47,252,253],{},"Business metrics",": number of inferences served, cost per 1,000 inferences",[18,256,257,258],{},"Implement data drift detection:\n",[15,259,260,263,266],{},[18,261,262],{},"Use Evidently AI or WhyLogs to monitor input feature distributions",[18,264,265],{},"Alert when drift exceeds a defined threshold",[18,267,268],{},"Document what drift means for model reliability and when retraining is needed",[18,270,271,272],{},"FinOps for AI:\n",[15,273,274,277,280],{},[18,275,276],{},"Calculate cost per 1,000 inference requests for both CPU and GPU serving",[18,278,279],{},"Implement scale-to-zero for non-critical model endpoints (using KEDA)",[18,281,282],{},"Document the cost-latency trade-off: when is GPU serving worth the cost?",[284,285,287],"h3",{"id":286},"stretch-goals","Stretch Goals",[15,289,290,293,296],{},[18,291,292],{},"Implement a full RAG (Retrieval-Augmented Generation) pipeline with a vector database (Qdrant or Weaviate) and measure infrastructure requirements",[18,294,295],{},"Set up a model fine-tuning pipeline that triggers automatically when drift is detected above threshold",[18,297,298],{},"Benchmark quantized models (GGUF/GGML) vs. full-precision serving and document the accuracy-cost trade-off",[284,300,302],{"id":301},"deliverables","Deliverables",[15,304,305,308,311,314,317,320],{},[18,306,307],{},"MLflow deployment with model registry, two registered model versions, and promotion criteria documented",[18,309,310],{},"Model serving deployment on Kubernetes with scaling, probes, and API gateway configured",[18,312,313],{},"GPU scheduling documentation (or live GPU deployment if available)",[18,315,316],{},"CI/CD pipeline for model deployment with canary rollout and automatic rollback",[18,318,319],{},"Grafana dashboard for model observability: request metrics, resource metrics, and drift alerts",[18,321,322],{},"FinOps report: cost per 1,000 inferences for CPU vs. GPU, scale-to-zero configuration",[284,324,326],{"id":325},"references","References",[38,328,329],{},[47,330,331],{},"Books",[15,333,334,348,355],{},[18,335,336,343,344],{},[337,338,342],"a",{"href":339,"rel":340},"https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/",[341],"nofollow","Designing Machine Learning Systems - Chip Huyen"," ",[345,346,347],"em",{},"(essential - the best book on ML in production)",[18,349,350],{},[337,351,354],{"href":352,"rel":353},"http://www.mlebook.com/",[341],"Machine Learning Engineering - Andriy Burkov",[18,356,357],{},[337,358,361],{"href":359,"rel":360},"https://www.oreilly.com/library/view/practical-mlops/9781098103002/",[341],"Practical MLOps - Noah Gift & Alfredo Deza",[38,363,364],{},[47,365,366],{},"Courses",[15,368,369,376,383],{},[18,370,371],{},[337,372,375],{"href":373,"rel":374},"https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops",[341],"Machine Learning Engineering for Production (MLOps) - Andrew Ng, Coursera",[18,377,378],{},[337,379,382],{"href":380,"rel":381},"https://fullstackdeeplearning.com/",[341],"Full Stack Deep Learning",[18,384,385],{},[337,386,389],{"href":387,"rel":388},"https://madewithml.com/",[341],"Made With ML - Goku Mohandas",[38,391,392],{},[47,393,394],{},"Tools and Documentation",[15,396,397,404,411,418,425,432,439,446],{},[18,398,399],{},[337,400,403],{"href":401,"rel":402},"https://mlflow.org/docs/latest/index.html",[341],"MLflow Documentation",[18,405,406],{},[337,407,410],{"href":408,"rel":409},"https://docs.bentoml.com/",[341],"BentoML Documentation",[18,412,413],{},[337,414,417],{"href":415,"rel":416},"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/",[341],"Triton Inference Server",[18,419,420],{},[337,421,424],{"href":422,"rel":423},"https://kserve.github.io/website/",[341],"KServe Documentation",[18,426,427],{},[337,428,431],{"href":429,"rel":430},"https://github.com/kedacore/http-add-on",[341],"KEDA HTTP Add-on",[18,433,434],{},[337,435,438],{"href":436,"rel":437},"https://docs.evidentlyai.com/",[341],"Evidently AI - ML Monitoring",[18,440,441],{},[337,442,445],{"href":443,"rel":444},"https://huggingface.co/models",[341],"Hugging Face Model Hub",[18,447,448],{},[337,449,452],{"href":450,"rel":451},"https://github.com/NVIDIA/k8s-device-plugin",[341],"NVIDIA Device Plugin for Kubernetes",[38,454,455],{},[47,456,457],{},"Articles",[15,459,460,470,477],{},[18,461,462,343,467],{},[337,463,466],{"href":464,"rel":465},"https://huyenchip.com/machine-learning-systems-design/toc.html",[341],"Chip Huyen: ML Systems Design Interview",[345,468,469],{},"(free)",[18,471,472],{},[337,473,476],{"href":474,"rel":475},"https://netflixtechblog.com/ml-platform-meetup-infra-for-contextual-bandits-and-reinforcement-learning-8a915eb13d4e",[341],"Netflix: ML Platform",[18,478,479],{},[337,480,483],{"href":481,"rel":482},"https://www.uber.com/blog/michelangelo-machine-learning-platform/",[341],"Uber: Michelangelo ML Platform",[485,486],"hr",{},[38,488,489],{},"Once you complete this task you will operate AI workloads with the same rigor as any other production service - with versioning, canary deployments, drift detection, and cost accountability. This is what separates a platform engineer who understands AI infrastructure from one who just deploys containers.",{"title":491,"searchDepth":492,"depth":492,"links":493},"",2,[494,495],{"id":12,"depth":492,"text":13},{"id":35,"depth":492,"text":36,"children":496},[497,499,500],{"id":286,"depth":498,"text":287},3,{"id":301,"depth":498,"text":302},{"id":325,"depth":498,"text":326},"Operate machine learning models in production with proper CI/CD, monitoring, and cost management for AI workloads","md",{},true,"/advanced/09_ai_infrastructure_mlops",{"title":5,"description":501},"advanced/09_ai_infrastructure_mlops",[509,510,511,512,513,514],"advanced","mlops","ai-infrastructure","kubernetes","gpu","devops","OfRAVftdpsOQ99Q2N0_eJoVRFytxRagfwJkrSlR3tzE",1772318814550]