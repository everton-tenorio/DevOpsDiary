<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>AI Infrastructure &amp; MLOps | DevOpsDiary</title><style>@import"https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Inter:wght@400;500;600&display=swap";body{background-color:#0a0e14;color:#000000a2;font-family:Inter,sans-serif;margin:0;padding:0}code,h1,h2,h3,h4,h5,h6,pre{font-family:JetBrains Mono,monospace}.terminal-glow{animation:glow 1.5s ease-in-out infinite alternate;box-shadow:0 0 10px #4ade8080}*{scrollbar-color:#4ade80 #1c2128;scrollbar-width:thin}::-webkit-scrollbar{height:6px;width:6px}::-webkit-scrollbar-track{background:#1c2128;border-radius:3px}::-webkit-scrollbar-thumb{background-color:#4ade80;border-radius:3px}.typing-cursor:after{animation:cursor-blink 1s step-end infinite;content:"|";display:inline-block}@keyframes cursor-blink{0%,to{opacity:1}50%{opacity:0}}@keyframes glow{0%{box-shadow:0 0 5px #4ade8080}to{box-shadow:0 0 20px #4ade80cc}}</style><style>@keyframes fadeIn-e02b96dc{0%{opacity:0;transform:translateY(-10px)}to{opacity:1;transform:translateY(0)}}</style><style>.project-content[data-v-549ebbdb] h2{font-family:JetBrains Mono,monospace;font-size:1.5rem;font-weight:700;line-height:2rem;margin-bottom:1rem;margin-top:1.5rem;--tw-text-opacity:1;color:rgb(74 222 128/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] h3{font-family:JetBrains Mono,monospace;font-size:1.25rem;line-height:1.75rem;margin-bottom:.5rem;margin-top:1rem;--tw-text-opacity:1;color:rgb(74 222 128/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] h4{font-size:1.125rem;line-height:1.75rem;padding-top:1rem;--tw-text-opacity:1;color:rgb(248 113 113/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] h5{font-size:1rem;line-height:1.5rem;padding-top:1rem;--tw-text-opacity:1;color:rgb(248 113 113/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] p{line-height:1.625;margin-bottom:1.5rem;margin-top:1rem}.project-content[data-v-549ebbdb] ul{list-style-type:disc;margin-bottom:1rem}.project-content[data-v-549ebbdb] ul>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.25rem*var(--tw-space-y-reverse));margin-top:calc(.25rem*(1 - var(--tw-space-y-reverse)))}.project-content[data-v-549ebbdb] ul{padding-left:1.5rem}.project-content[data-v-549ebbdb] ol{list-style-type:decimal;margin-bottom:1rem}.project-content[data-v-549ebbdb] ol>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.25rem*var(--tw-space-y-reverse));margin-top:calc(.25rem*(1 - var(--tw-space-y-reverse)))}.project-content[data-v-549ebbdb] ol{padding-left:1.5rem}.project-content[data-v-549ebbdb] a{--tw-text-opacity:1;color:rgb(149 186 253/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] a:hover{text-decoration-line:underline}.project-content[data-v-549ebbdb] pre{border-radius:.25rem;margin-bottom:1rem;overflow-x:auto;--tw-bg-opacity:1;background-color:rgb(26 26 26/var(--tw-bg-opacity,1));font-family:JetBrains Mono,monospace;padding:1rem;--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] code{border-radius:.25rem;--tw-bg-opacity:1;background-color:rgb(26 26 26/var(--tw-bg-opacity,1));font-family:JetBrains Mono,monospace;font-size:.875rem;line-height:1.25rem;padding:.125rem .375rem;--tw-text-opacity:1;color:rgb(134 239 172/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] pre code{background-color:transparent;padding:0;--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.project-content[data-v-549ebbdb] pre code:after,.project-content[data-v-549ebbdb] pre code:before{content:none!important}.project-content[data-v-549ebbdb] blockquote{border-left-width:4px;margin-bottom:1rem;margin-top:1rem;--tw-border-opacity:1;border-color:rgb(74 222 128/var(--tw-border-opacity,1));font-style:italic;padding-left:1rem;--tw-text-opacity:1;color:rgb(209 213 219/var(--tw-text-opacity,1))}a[data-v-549ebbdb]:focus,button[data-v-549ebbdb]:focus{outline:none}</style><style>.popup-enter-active[data-v-5bb999b5]{transition:opacity .3s ease,transform .35s cubic-bezier(.34,1.56,.64,1)}.popup-leave-active[data-v-5bb999b5]{transition:opacity .2s ease,transform .2s ease}.popup-enter-from[data-v-5bb999b5]{opacity:0;transform:scale(.94)}.popup-leave-to[data-v-5bb999b5]{opacity:0;transform:scale(.97)}</style><link rel="stylesheet" href="/_nuxt/entry.CPXZsUu4.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/advanced/09_ai_infrastructure_mlops/_payload.json?462c6198-17a7-4e39-9109-90b83459aafd"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B6QmmOke.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DFbZ2x5o.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D-rYVxCD.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BMZ4RdS_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/KKNTyPo_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DEq9J3pr.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D3W9tIMM.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Dw7pLMPy.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BU7ptq2r.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/YD_6zvBs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/1ebc_2W_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Db4S2A2Z.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DmsM7HfT.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CFdE243-.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D8OxvjdY.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/G3XGfLAu.js"><script type="module" src="/_nuxt/B6QmmOke.js" crossorigin></script><meta name="keywords" content="DevOps, CI/CD, Docker, Kubernetes, Terraform, Ansible, GitOps, IaC, DevSecOps, SRE"><meta property="og:title" content="#DevOpsDiary"><meta property="og:description" content="Hands-on DevOps projects for all skill levels."><meta property="og:image" content="https://devopsdiary.site/devopsdiary.png"><meta property="og:url" content="https://devopsdiary.site"><meta property="og:type" content="website"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:title" content="#DevOpsDiary"><meta property="twitter:description" content="Hands-on DevOps projects for all skill levels."><meta property="twitter:image" content="https://devopsdiary.site/devopsdiary.png"><link rel="canonical" href="https://devopsdiary.site"><meta name="description" content="Operate machine learning models in production with proper CI/CD, monitoring, and cost management for AI workloads"></head><body><div id="__nuxt"><div class="bg-[#0A0E14] min-h-screen flex flex-col font-sans relative overflow-hidden"><div class="absolute inset-0 opacity-10 pointer-events-none bg-[length:40px_40px] bg-[linear-gradient(to_right,#4ADE8015_1px,transparent_1px),linear-gradient(to_bottom,#4ADE8015_1px,transparent_1px)]"></div><header class="bg-[#161b22] border-b border-green-400/30 sticky top-0 z-50 backdrop-blur-sm" data-v-e02b96dc><div class="container mx-auto px-4 py-3" data-v-e02b96dc><div class="flex flex-wrap items-center justify-between" data-v-e02b96dc><a href="/" class="text-xl font-mono font-bold text-green-400 hover:text-white transition-colors duration-200" data-v-e02b96dc><img src="/devopsdiary1.png" width="60px" data-v-e02b96dc></a><button class="md:hidden p-2 text-green-400 border border-green-400/30 rounded hover:bg-green-400 hover:text-black transition-colors" data-v-e02b96dc><span data-v-e02b96dc>Menu</span></button><nav class="hidden md:flex items-center space-x-6" data-v-e02b96dc><!--[--><a href="/" class="font-mono text-white hover:text-green-400 border-b-2 border-transparent hover:border-green-400 pb-1 transition-all duration-200" data-v-e02b96dc>Home</a><a href="/blog" class="font-mono text-white hover:text-green-400 border-b-2 border-transparent hover:border-green-400 pb-1 transition-all duration-200" data-v-e02b96dc>Blog</a><a href="/about" class="font-mono text-white hover:text-green-400 border-b-2 border-transparent hover:border-green-400 pb-1 transition-all duration-200" data-v-e02b96dc>About</a><!--]--><iframe src="https://github.com/sponsors/everton-tenorio/button" title="Sponsor everton-tenorio" height="32" width="114" style="border:0;border-radius:6px;margin-top:-5px;" data-v-e02b96dc></iframe></nav></div><!----></div></header><main class="flex-grow container mx-auto px-4 py-8 relative z-10"><div class="max-w-3xl mx-auto" data-v-549ebbdb><div class="mb-8" data-v-549ebbdb><h1 class="text-3xl font-mono font-bold text-green-400 mb-4" data-v-549ebbdb>AI Infrastructure &amp; MLOps</h1><div class="mb-4 flex flex-wrap gap-2" data-v-549ebbdb><!--[--><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>advanced</span><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>mlops</span><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>ai-infrastructure</span><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>kubernetes</span><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>gpu</span><span class="inline-block bg-gray-800 text-green-400 px-2 py-1 rounded font-mono text-sm" data-v-549ebbdb>devops</span><!--]--></div><p class="text-[#E6EDF3] text-lg mb-4" data-v-549ebbdb>Operate machine learning models in production with proper CI/CD, monitoring, and cost management for AI workloads</p><a href="/advanced" class="text-green-400 hover:underline font-mono" data-v-549ebbdb> ← Back to Advanced</a></div><div class="bg-[#1c2128] text-[#E6EDF3] lg:mx-0 -mx-5 p-6 rounded-lg mb-8 project-content" data-v-549ebbdb><div data-v-549ebbdb><h2 id="learning-objectives"><a href="#learning-objectives"><!--[-->Learning Objectives<!--]--></a></h2><ul><!--[--><li><!--[-->Understand the MLOps lifecycle and how it extends traditional DevOps practices<!--]--></li><li><!--[-->Serve machine learning models in production with Kubernetes and GPU scheduling<!--]--></li><li><!--[-->Build CI/CD pipelines for model versioning, testing, and deployment<!--]--></li><li><!--[-->Monitor model performance in production: latency, throughput, drift, and cost<!--]--></li><li><!--[-->Manage the cost of GPU compute as an engineering discipline<!--]--></li><!--]--></ul><h2 id="requirements"><a href="#requirements"><!--[-->Requirements<!--]--></a></h2><p><!--[-->You are required to build and operate the infrastructure for running AI/ML workloads in production. You do not need to train models - the focus is entirely on operating them:<!--]--></p><ol><!--[--><li><!--[--><strong><!--[-->MLOps Architecture and Model Registry<!--]--></strong><ul><!--[--><li><!--[-->Deploy MLflow as a model registry and experiment tracker:
<ul><!--[--><li><!--[-->Configure a PostgreSQL backend for metadata<!--]--></li><li><!--[-->Configure S3 or GCS for artifact storage (model files, datasets)<!--]--></li><li><!--[-->Register at least two versions of a pre-trained model (use any public model from Hugging Face)<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Document the MLOps lifecycle:
<ul><!--[--><li><!--[-->Model development → experiment tracking → model registration → staging → production<!--]--></li><li><!--[-->How this differs from and integrates with the existing software delivery pipeline<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Define model promotion criteria:
<ul><!--[--><li><!--[-->A model is promoted to production only if: accuracy &gt; baseline, latency p95 &lt; 200ms, no regression on a validation dataset<!--]--></li><!--]--></ul><!--]--></li><!--]--></ul><!--]--></li><li><!--[--><strong><!--[-->Model Serving Infrastructure<!--]--></strong><ul><!--[--><li><!--[-->Deploy a model serving stack on Kubernetes:
<ul><!--[--><li><!--[--><strong><!--[-->Option A<!--]--></strong>: BentoML - package and serve a Hugging Face model as an HTTP API<!--]--></li><li><!--[--><strong><!--[-->Option B<!--]--></strong>: Triton Inference Server - serve an ONNX or TensorRT model<!--]--></li><li><!--[--><strong><!--[-->Option C<!--]--></strong>: Seldon Core or KServe - full model serving platform on Kubernetes<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Configure the serving deployment with:
<ul><!--[--><li><!--[-->Readiness and liveness probes<!--]--></li><li><!--[-->Resource requests/limits (CPU for non-GPU workloads; document GPU requirements if relevant)<!--]--></li><li><!--[-->Horizontal scaling based on inference request queue depth (KEDA)<!--]--></li><li><!--[-->Rolling deployment strategy for model updates<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Expose the model via an API Gateway with rate limiting and authentication<!--]--></li><li><!--[-->Verify the serving endpoint: send inference requests and validate responses<!--]--></li><!--]--></ul><!--]--></li><li><!--[--><strong><!--[-->GPU Scheduling (Conceptual + Practical where possible)<!--]--></strong><ul><!--[--><li><!--[-->If GPU nodes are available (GKE Autopilot with GPU, AWS EC2 g4dn, or local GPU):
<ul><!--[--><li><!--[-->Deploy the NVIDIA Device Plugin for Kubernetes<!--]--></li><li><!--[-->Schedule a model inference Pod with GPU resource requests (<code><!--[-->nvidia.com/gpu: 1<!--]--></code>)<!--]--></li><li><!--[-->Measure inference latency on GPU vs. CPU<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->If GPU nodes are not available:
<ul><!--[--><li><!--[-->Document the complete GPU scheduling setup in <code><!--[-->gpu-infrastructure.md<!--]--></code><!--]--></li><li><!--[-->Use a CPU-optimized quantized model (GGUF format with llama.cpp or similar) as a substitute<!--]--></li><li><!--[-->Document cost comparison: GPU instance cost vs. CPU instance cost for equivalent throughput<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Configure time-slicing or MIG (Multi-Instance GPU) for shared GPU utilization documentation<!--]--></li><!--]--></ul><!--]--></li><li><!--[--><strong><!--[-->CI/CD for Models<!--]--></strong><ul><!--[--><li><!--[-->Implement a model deployment pipeline triggered on model registry promotion:
<ul><!--[--><li><!--[-->Pull the new model version from MLflow registry<!--]--></li><li><!--[-->Run automated evaluation: inference latency test, accuracy on a held-out dataset<!--]--></li><li><!--[-->Canary deploy: route 10% of traffic to the new model, 90% to the current production model<!--]--></li><li><!--[-->Monitor for 30 minutes: if no degradation, promote to 100%<!--]--></li><li><!--[-->Automatic rollback if p95 latency degrades &gt; 30% or error rate &gt; 1%<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Implement model versioning in the serving layer:
<ul><!--[--><li><!--[-->Multiple model versions can run simultaneously<!--]--></li><li><!--[-->Traffic split is configurable without redeployment<!--]--></li><li><!--[-->Each version is independently observable<!--]--></li><!--]--></ul><!--]--></li><!--]--></ul><!--]--></li><li><!--[--><strong><!--[-->Model Observability and Cost Management<!--]--></strong><ul><!--[--><li><!--[-->Deploy a monitoring stack for model inference:
<ul><!--[--><li><!--[--><strong><!--[-->Request metrics<!--]--></strong>: requests/second, p50/p95/p99 latency, error rate per model version<!--]--></li><li><!--[--><strong><!--[-->Resource metrics<!--]--></strong>: CPU/GPU utilization, memory usage, batch size distribution<!--]--></li><li><!--[--><strong><!--[-->Business metrics<!--]--></strong>: number of inferences served, cost per 1,000 inferences<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Implement data drift detection:
<ul><!--[--><li><!--[-->Use Evidently AI or WhyLogs to monitor input feature distributions<!--]--></li><li><!--[-->Alert when drift exceeds a defined threshold<!--]--></li><li><!--[-->Document what drift means for model reliability and when retraining is needed<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->FinOps for AI:
<ul><!--[--><li><!--[-->Calculate cost per 1,000 inference requests for both CPU and GPU serving<!--]--></li><li><!--[-->Implement scale-to-zero for non-critical model endpoints (using KEDA)<!--]--></li><li><!--[-->Document the cost-latency trade-off: when is GPU serving worth the cost?<!--]--></li><!--]--></ul><!--]--></li><!--]--></ul><!--]--></li><!--]--></ol><h3 id="stretch-goals"><a href="#stretch-goals"><!--[-->Stretch Goals<!--]--></a></h3><ul><!--[--><li><!--[-->Implement a full RAG (Retrieval-Augmented Generation) pipeline with a vector database (Qdrant or Weaviate) and measure infrastructure requirements<!--]--></li><li><!--[-->Set up a model fine-tuning pipeline that triggers automatically when drift is detected above threshold<!--]--></li><li><!--[-->Benchmark quantized models (GGUF/GGML) vs. full-precision serving and document the accuracy-cost trade-off<!--]--></li><!--]--></ul><h3 id="deliverables"><a href="#deliverables"><!--[-->Deliverables<!--]--></a></h3><ul><!--[--><li><!--[-->MLflow deployment with model registry, two registered model versions, and promotion criteria documented<!--]--></li><li><!--[-->Model serving deployment on Kubernetes with scaling, probes, and API gateway configured<!--]--></li><li><!--[-->GPU scheduling documentation (or live GPU deployment if available)<!--]--></li><li><!--[-->CI/CD pipeline for model deployment with canary rollout and automatic rollback<!--]--></li><li><!--[-->Grafana dashboard for model observability: request metrics, resource metrics, and drift alerts<!--]--></li><li><!--[-->FinOps report: cost per 1,000 inferences for CPU vs. GPU, scale-to-zero configuration<!--]--></li><!--]--></ul><h3 id="references"><a href="#references"><!--[-->References<!--]--></a></h3><p><!--[--><strong><!--[-->Books<!--]--></strong><!--]--></p><ul><!--[--><li><!--[--><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/" rel="nofollow"><!--[-->Designing Machine Learning Systems - Chip Huyen<!--]--></a> <em><!--[-->(essential - the best book on ML in production)<!--]--></em><!--]--></li><li><!--[--><a href="http://www.mlebook.com/" rel="nofollow"><!--[-->Machine Learning Engineering - Andriy Burkov<!--]--></a><!--]--></li><li><!--[--><a href="https://www.oreilly.com/library/view/practical-mlops/9781098103002/" rel="nofollow"><!--[-->Practical MLOps - Noah Gift &amp; Alfredo Deza<!--]--></a><!--]--></li><!--]--></ul><p><!--[--><strong><!--[-->Courses<!--]--></strong><!--]--></p><ul><!--[--><li><!--[--><a href="https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops" rel="nofollow"><!--[-->Machine Learning Engineering for Production (MLOps) - Andrew Ng, Coursera<!--]--></a><!--]--></li><li><!--[--><a href="https://fullstackdeeplearning.com/" rel="nofollow"><!--[-->Full Stack Deep Learning<!--]--></a><!--]--></li><li><!--[--><a href="https://madewithml.com/" rel="nofollow"><!--[-->Made With ML - Goku Mohandas<!--]--></a><!--]--></li><!--]--></ul><p><!--[--><strong><!--[-->Tools and Documentation<!--]--></strong><!--]--></p><ul><!--[--><li><!--[--><a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"><!--[-->MLflow Documentation<!--]--></a><!--]--></li><li><!--[--><a href="https://docs.bentoml.com/" rel="nofollow"><!--[-->BentoML Documentation<!--]--></a><!--]--></li><li><!--[--><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/" rel="nofollow"><!--[-->Triton Inference Server<!--]--></a><!--]--></li><li><!--[--><a href="https://kserve.github.io/website/" rel="nofollow"><!--[-->KServe Documentation<!--]--></a><!--]--></li><li><!--[--><a href="https://github.com/kedacore/http-add-on" rel="nofollow"><!--[-->KEDA HTTP Add-on<!--]--></a><!--]--></li><li><!--[--><a href="https://docs.evidentlyai.com/" rel="nofollow"><!--[-->Evidently AI - ML Monitoring<!--]--></a><!--]--></li><li><!--[--><a href="https://huggingface.co/models" rel="nofollow"><!--[-->Hugging Face Model Hub<!--]--></a><!--]--></li><li><!--[--><a href="https://github.com/NVIDIA/k8s-device-plugin" rel="nofollow"><!--[-->NVIDIA Device Plugin for Kubernetes<!--]--></a><!--]--></li><!--]--></ul><p><!--[--><strong><!--[-->Articles<!--]--></strong><!--]--></p><ul><!--[--><li><!--[--><a href="https://huyenchip.com/machine-learning-systems-design/toc.html" rel="nofollow"><!--[-->Chip Huyen: ML Systems Design Interview<!--]--></a> <em><!--[-->(free)<!--]--></em><!--]--></li><li><!--[--><a href="https://netflixtechblog.com/ml-platform-meetup-infra-for-contextual-bandits-and-reinforcement-learning-8a915eb13d4e" rel="nofollow"><!--[-->Netflix: ML Platform<!--]--></a><!--]--></li><li><!--[--><a href="https://www.uber.com/blog/michelangelo-machine-learning-platform/" rel="nofollow"><!--[-->Uber: Michelangelo ML Platform<!--]--></a><!--]--></li><!--]--></ul><hr><p><!--[-->Once you complete this task you will operate AI workloads with the same rigor as any other production service - with versioning, canary deployments, drift detection, and cost accountability. This is what separates a platform engineer who understands AI infrastructure from one who just deploys containers.<!--]--></p></div></div><div class="bg-[#1c2128] text-[#E6EDF3] p-6 rounded-lg mb-8" data-v-549ebbdb><h2 class="text-xl font-mono font-bold text-green-400 mb-4" data-v-549ebbdb>Submit Your Solution</h2><p class="mb-4" data-v-549ebbdb>Completed this project? Share your solution with the community!</p><ol class="list-decimal pl-6 mb-6 space-y-2" data-v-549ebbdb><li data-v-549ebbdb>Push your code to a GitHub repository</li><li data-v-549ebbdb>Open an issue on our GitHub repo with your solution link</li><li data-v-549ebbdb>Share on X with the hashtag #DevOpsDiary</li></ol><div class="flex flex-wrap gap-4" data-v-549ebbdb><a href="https://github.com/everton-tenorio/DevOpsDiary/issues/new?template=project_submission.yml&amp;title=&amp;project_title=AI%20Infrastructure%20%26%20MLOps&amp;labels=advanced&amp;level=Advanced&amp;devopsdiary_link=https%3A%2F%2Fdevopsdiary.site%2Fadvanced%2F09_ai_infrastructure_mlops&amp;repository_link=&amp;description=" target="_blank" rel="noopener noreferrer" class="inline-block bg-green-400 text-gray-900 px-4 py-2 rounded font-mono hover:bg-green-300 transition-colors" data-v-549ebbdb> Submit Solution </a><a href="https://twitter.com/intent/tweet?hashtags=DevOpsDiary" target="_blank" rel="noopener noreferrer" class="inline-block bg-green-400 text-gray-900 px-4 py-2 rounded font-mono hover:bg-green-300 transition-colors" data-v-549ebbdb> Share on X </a></div></div></div></main><footer class="bg-[#161b22] border-t border-green-400/30 py-8 relative overflow-hidden"><div class="container mx-auto px-4"><div class="max-w-3xl mx-auto bg-[#0A0E14] rounded-lg border border-green-400/30 overflow-hidden shadow-lg"><div class="bg-[#1c2128] px-4 py-2 flex items-center"><div class="flex space-x-2 mr-4"><div class="w-3 h-3 rounded-full bg-red-500"></div><div class="w-3 h-3 rounded-full bg-yellow-500"></div><div class="w-3 h-3 rounded-full bg-green-500"></div></div><div class="font-mono text-xs text-gray-400">~/devops-diary</div></div><div class="p-6 font-mono"><div class="mb-6 text-green-400"><span class="text-blue-400">$</span> <span class="inline-block w-2 h-4 bg-green-400 ml-1 animate-pulse"></span></div><div class="flex flex-col md:flex-row md:items-center justify-between py-4 border-t border-green-400/20"><div class="flex flex-wrap gap-4 mb-6 md:mb-0"><a href="https://github.com/everton-tenorio/DevOpsDiary" target="_blank" rel="noopener noreferrer" class="text-white hover:text-green-400 transition-colors duration-200 flex items-center"><svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 2C6.477 2 2 6.477 2 12c0 4.418 2.865 8.167 6.84 9.651.5.1.712-.216.712-.499 0-.254-.008-1-.008-1.834C7 19.91 6.35 18.781 6.15 18.201c-.113-.275-.6-1.181-1.025-1.435C4.775 16.566 4.275 15.974 5.125 15.964c.775-.01 1.338.767 1.525 1.052.9 1.539 2.338 1.224 2.934.941.105-.671.3-1.137.566-1.383-2.375-.246-4.85-1.103-4.85-4.973 0-1.144.412-2.092 1.375-2.842-.116-.273-.488-1.337.125-2.778 0 0 .837-.283 2.75.948a9.4 9.4 0 012.513-.337c.85 0 1.7.127 2.513.337 1.913-1.231 2.75-.948 2.75-.948.613 1.441.241 2.505.125 2.778.963.75 1.375 1.698 1.375 2.842 0 3.88-2.475 4.727-4.85 4.973.375.313.7.905.7 1.839 0 1.368-.025 2.386-.025 2.763 0 .289.237.614.737.5C19.137 20.174 22 16.576 22 12.226 22 6.703 17.523 2.226 12 2.226 6.477 2.226 2 6.478 2 12c0 5.522 4.477 10 10 10 5.523 0 10-4.478 10-10C22 6.478 17.523 2 12 2z"></path></svg><span>GitHub</span></a><a href="https://twitter.com/hashtag/DevOpsDiary" target="_blank" rel="noopener noreferrer" class="text-white hover:text-green-400 transition-colors duration-200 flex items-center"><svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg><span>#DevOpsDiary</span></a><iframe src="https://github.com/sponsors/everton-tenorio/button" title="Sponsor everton-tenorio" height="32" width="114" style="border:0;border-radius:6px;"></iframe></div><div class="text-gray-400 text-sm font-mono"><span class="text-blue-400">©</span> 2026 DevOpsDiary </div></div></div></div></div></footer><!----></div></div><div id="teleports"></div><script>window.__NUXT__={};window.__NUXT__.config={public:{mdc:{components:{prose:true,map:{},customElements:[]},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}},highlight:{noApiRoute:true,highlighter:"shiki",theme:{default:"github-light",dark:"github-dark"},shikiEngine:"oniguruma",langs:["js","jsx","json","ts","tsx","vue","css","html","bash","md","mdc","yaml"]}},content:{wsUrl:""}},app:{baseURL:"/",buildId:"462c6198-17a7-4e39-9109-90b83459aafd",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/advanced/09_ai_infrastructure_mlops/_payload.json?462c6198-17a7-4e39-9109-90b83459aafd">[{"state":1,"once":3,"_errors":4,"serverRendered":6,"path":7,"prerenderedAt":8},["Reactive",2],{},["Set"],["ShallowReactive",5],{"advanced-09_ai_infrastructure_mlops":-1},true,"/advanced/09_ai_infrastructure_mlops",1772318814550]</script></body></html>